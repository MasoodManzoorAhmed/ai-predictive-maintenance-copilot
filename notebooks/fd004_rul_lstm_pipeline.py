# -*- coding: utf-8 -*-
"""FD004_RUL_LSTM_Pipeline.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1eBq1modCjzDRauSXJONjDzpCczZe0edQ

# **1. Setup and Imports**

This section initializes the execution environment for the FD004 CMAPSS pipeline.
Key tasks include reproducibility seeding, optional Google Drive mounting, and validation of dataset file paths.
Ensuring correct file locations early prevents silent downstream failures in data loading, preprocessing, and model training.
"""

# 1. Setup and Imports

import os
import random
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Simple helper for consistent console logs
def log(msg: str):
    print(f"[INFO] {msg}")

# Reproducibility settings
SEED = 42
random.seed(SEED)
np.random.seed(SEED)

log(f"Seed fixed at {SEED} for reproducibility.")

# Google Drive mount when running in Colab
try:
    from google.colab import drive
    log("Colab environment detected. Mounting Google Drive...")
    drive.mount('/content/drive')
    log("Drive mount complete")
except ImportError:
    log("Not running in Colab — skipping drive.mount().")

# Base directory for FD004 dataset
BASE_PATH_FD004 = "/content/drive/MyDrive/OIL PREDICTIVE MAINTAINANCE/CMaps"

train_path_fd004 = os.path.join(BASE_PATH_FD004, "train_FD004.txt")
test_path_fd004  = os.path.join(BASE_PATH_FD004, "test_FD004.txt")
rul_path_fd004   = os.path.join(BASE_PATH_FD004, "RUL_FD004.txt")

log("FD004 expected file paths:")
print("   Train:", train_path_fd004)
print("   Test :", test_path_fd004)
print("   RUL  :", rul_path_fd004)

# Quick sanity check for missing files
missing = []
for p in [train_path_fd004, test_path_fd004, rul_path_fd004]:
    if os.path.exists(p):
        print(f"[FD004][OK]   Found: {p}")
    else:
        print(f"[FD004][MISS] Missing: {p}")
        missing.append(p)

if missing:
    raise FileNotFoundError(
        "ERROR: Required FD004 files are missing:\n" +
        "\n".join(missing)
    )

log("Setup and path validation complete.")

"""# **2. Define FD004 File Paths**

All FD004 paths are standardized to maintain alignment with FD001–FD003 pipelines.
Using a central base directory ensures consistency, portability, and minimal refactoring when migrating notebooks or running in different environments.
"""

# 2. Load FD004 Raw Data


# Base directory for all FD004 CMAPSS files.
base_path_fd004 = BASE_PATH_FD004

train_path_fd004 = f"{base_path_fd004}/train_FD004.txt"
test_path_fd004  = f"{base_path_fd004}/test_FD004.txt"
rul_path_fd004   = f"{base_path_fd004}/RUL_FD004.txt"

print("Paths set for FD004:")
print("  Train:", train_path_fd004)
print("  Test :", test_path_fd004)
print("  RUL  :", rul_path_fd004)

"""# **3. Load Raw Dataset Files (FD004)**

Raw CMAPSS data for FD004 is loaded using the NASA-specified column schema.
This cell produces the base train/test DataFrames from which all further preprocessing, feature engineering, and sequence construction will be derived.
"""

# 3. LOAD RAW FD004 DATA

# cmaps layout for fd004: id, settings, then sensors
fd004_cols = ["unit", "cycle", "setting1", "setting2", "setting3"] + [
    f"sensor{i}" for i in range(1, 22)
]

train_fd004 = pd.read_csv(train_path_fd004, sep=r"\s+", header=None, names=fd004_cols)
test_fd004  = pd.read_csv(test_path_fd004,  sep=r"\s+", header=None, names=fd004_cols)
# rul file is just a single column
rul_fd004   = pd.read_csv(rul_path_fd004,   sep=r"\s+", header=None)[0]

print("FD004 Train Shape:", train_fd004.shape)
print("FD004 Test Shape :", test_fd004.shape)
print("FD004 RUL Shape  :", rul_fd004.shape)
print("\nfd004 train head:")
display(train_fd004.head())

"""# **4. Quick sanity checks on fd004 data**

Quick look at structure, missing values, duplicates, and unit counts for train/test.
Nothing fancy here, just making sure the raw load is not broken before going deeper.
"""

# 4.basic structure / quality checks on fd004 train + test


print("=== fd004 train basic info ===")
display(train_fd004.head())
print("\ntrain shape:", train_fd004.shape)

print("\n Data types:")
print(train_fd004.dtypes)

print("\n Missing values (train):")
print(train_fd004.isna().sum())

print("\n Duplicate rows in TRAIN:", train_fd004.duplicated().sum())
print(" Duplicate rows in TEST :", test_fd004.duplicated().sum())

print("\n=== ENGINE COUNTS (FD004) ===")
print("Unique engines in TRAIN:", train_fd004["unit"].nunique())
print("Unique engines in TEST :", test_fd004["unit"].nunique())

print("\nsample test rows:")
display(test_fd004.head())

print("\nfd004 basic checks done, nothing obviously broken.")

"""# **5. PER-ENGINE CYCLE STATS & VISUALIZATION (FD004)**

Checking how many cycles each engine runs before failure in FD004.
Gives a feel for run-length spread and how many very short sequences exist.
"""

# 5. PER-ENGINE CYCLE STATS & VISUALIZATION (FD004)
# look at max cycle per engine to understand run lengths

log("checking per-engine cycle stats for FD004...")
print("per-engine max cycle stats:")
cycles_per_engine_fd004 = train_fd004.groupby("unit")["cycle"].max()
display(cycles_per_engine_fd004.describe())

plt.figure(figsize=(8,5))
sns.histplot(cycles_per_engine_fd004, bins=20, kde=True)
plt.title("FD004 — Distribution of Max Cycles per Engine (TRAIN)")
plt.xlabel("Max cycles per engine")
plt.ylabel("Count of engines")
plt.grid(True)
plt.show()

print("\nMin cycles:", cycles_per_engine_fd004.min())
print("Max cycles:", cycles_per_engine_fd004.max())
print("Engines with very short runs (<=50 cycles):",
      (cycles_per_engine_fd004 <= 50).sum())

log("per-engine cycle distribution plotted")

"""# **6. Settings distribution (fd004)**

Quick look at the three operating settings in FD004.
Just checking ranges and distributions before worrying about sensors.
"""

# 6. SETTINGS DISTRIBUTION & BASIC VISUAL CHECKS (FD004)

# basic summary + histograms for the 3 operating settings
setting_cols_fd004 = ["setting1", "setting2", "setting3"]

print("FD004 — SETTINGS SUMMARY (TRAIN)")
display(train_fd004[setting_cols_fd004].describe())

fig, axes = plt.subplots(1, 3, figsize=(15,4))
for i, col in enumerate(setting_cols_fd004):
    sns.histplot(train_fd004[col], bins=30, kde=True, ax=axes[i])
    axes[i].set_title(f"Distribution of {col}")
    axes[i].grid(True)
plt.tight_layout()
plt.show()

print("\n Settings look numerically sensible for FD004 (no NaNs, no obvious out-of-range spikes).")

"""# **7. SENSOR STATS, NEAR-CONSTANT DETECTION & CORRELATION**

A statistical overview of sensor signals is performed to identify near-constant sensors and redundant features.
Low-variance sensors are typically non-informative for RUL modeling and can degrade neural sequence model performance if not removed.
"""

# 7. SENSOR STATS, NEAR-CONSTANT DETECTION & CORRELATION


# sensors only (leave settings alone)
sensor_cols_fd004 = [c for c in train_fd004.columns if c.startswith("sensor")]

print("FD004 sensor summary (train, first 15):")
display(train_fd004[sensor_cols_fd004].describe().T.head(15))

# Standard deviation of each sensor
sensor_std_fd004 = train_fd004[sensor_cols_fd004].std()
print("\n Sensor standard deviations (first 15):")
display(sensor_std_fd004.head(15))

# Threshold to detect practically constant sensors
STD_THRESHOLD = 1e-6
constant_cols_fd004 = sensor_std_fd004[sensor_std_fd004 < STD_THRESHOLD].index.tolist()

print(f"\n Near-constant sensor columns in FD004 (std < {STD_THRESHOLD}):")
print(constant_cols_fd004)

# Quick correlation heatmap for first 10 sensors (before dropping)
subset_sensors_fd004 = sensor_cols_fd004[:10]
plt.figure(figsize=(10, 8))
corr_fd004 = train_fd004[subset_sensors_fd004].corr()
sns.heatmap(corr_fd004, annot=False, cmap="coolwarm", square=True)
plt.title("FD004 — Correlation Heatmap (First 10 Sensors)")
plt.show()

print(f"\n{len(sensor_cols_fd004)} sensors total, {len(constant_cols_fd004) or 0} near-constant.")

"""# **8. DROP NEAR-CONSTANT SENSORS (FD004)**

Removing sensors with std < 1e-6 from both train and test.
Settings stay untouched, only sensors get cleaned.
"""

# 8. DROP NEAR-CONSTANT SENSORS (FD004)


print("Dropping near-constant sensor columns (FD004):", constant_cols_fd004)

# Drop only from sensor columns (settings stay untouched)
if constant_cols_fd004:
    train_fd004.drop(columns=constant_cols_fd004, inplace=True)
    test_fd004.drop(columns=constant_cols_fd004, inplace=True)
else:
    print("No near-constant sensors detected — nothing to drop.")

# Recompute sensor_cols after dropping
sensor_cols_fd004 = [c for c in train_fd004.columns if c.startswith("sensor")]

print("\n After dropping near-constant sensors (FD004):")
print("Train shape:", train_fd004.shape)
print("Test  shape:", test_fd004.shape)
print("Remaining sensors:", len(sensor_cols_fd004))
print(sensor_cols_fd004)

"""# **9. GENERATE RUL FOR TRAINING DATA (FD004)**

Calculate remaining useful life (RUL) as max cycle minus current cycle per unit.
This is the target variable for training.

- For each engine (`unit`), find its **maximum cycle** in the training data.
- Compute **Remaining Useful Life (RUL)** for every row as:
  \[
  \text{RUL} = \text{max\_cycle (for that engine)} - \text{current cycle}
  \]
- Add two new columns to `train_fd004`:
  - `max_cycle` – final cycle for that engine.
  - `RUL` – remaining cycles until failure at each time step.
- Show a small sample of `unit`, `cycle`, `max_cycle`, and `RUL` to confirm the logic.
- Plot the **RUL distribution** before any capping to understand how many early vs late points we have.
- Sanity check:
  - For every engine, the **last row** should have `RUL = 0`.  
  - Print the unique RUL values at the last cycles to verify this.
"""

# CELL 9 — GENERATE RUL FOR TRAINING DATA (FD004)


log("Generating RUL labels for FD004 training data...")

# For each engine: RUL = max_cycle - cycle

# max cycle per engine in training data
max_cycle_per_unit_fd004 = train_fd004.groupby("unit")["cycle"].max()

# map max cycle back to each row
train_fd004["max_cycle"] = train_fd004["unit"].map(max_cycle_per_unit_fd004)

# RUL = max cycle - current cycle
train_fd004["RUL"] = train_fd004["max_cycle"] - train_fd004["cycle"]

print("Sample RUL values (FD004) ")
display(train_fd004[["unit", "cycle", "max_cycle", "RUL"]].head(10))

print("\nRUL stats before capping")
display(train_fd004["RUL"].describe())

plt.figure(figsize=(8,5))
sns.histplot(train_fd004["RUL"], bins=40, kde=True)
plt.title("FD004 — RUL Distribution (Before Capping)")
plt.xlabel("RUL")
plt.ylabel("Count")
plt.grid(True)
plt.show()

# Sanity: last cycle per engine has RUL = 0
check_last_fd004 = train_fd004.groupby("unit").tail(1)
print("\nSanity check — RUL at last cycle should be 0 (FD004):")
display(check_last_fd004[["unit", "cycle", "max_cycle", "RUL"]].head())
print("Unique RUL at last cycles:", check_last_fd004["RUL"].unique()[:10])

log("RUL generation completed.")

"""# **10. Apply RUL Cap and Visualize Distribution(FD004)**

- To stabilize training and reduce extreme outliers, clip all RUL values using:
  \[
  \text{RUL} = \min(\text{RUL},\,125)
  \]
- This ensures the target values stay within a manageable range for sequence models.
- After applying the cap:
  - Display summary statistics to verify the new distribution.
  - Plot a histogram (with KDE) to visualize how the capped RUL values are spread.
- This step helps the model focus on learning realistic degradation patterns without being dominated by very large RUL values.
"""

# 10. APPLY RUL CAP (125) & VISUALIZE (FD004)


RUL_CAP_FD004 = 125

# cap RUL values
train_fd004["RUL"] = np.minimum(train_fd004["RUL"], RUL_CAP_FD004)

print(f"Applied RUL cap of {RUL_CAP_FD004} for FD004.")
print("RUL stats after capping:")
display(train_fd004["RUL"].describe())

plt.figure(figsize=(8,5))
sns.histplot(train_fd004["RUL"], bins=40, kde=True)
plt.title(f"FD004 — RUL Distribution (After Capping at {RUL_CAP_FD004})")
plt.xlabel("RUL (capped)")
plt.ylabel("Count")
plt.grid(True)
plt.show()

log("RUL capping and plotting done.")

"""# **11. FEATURE COLUMN ANALYSIS & CONSTANT COLUMN REMOVAL**

This block finalizes feature selection by removing columns with no variation and defining the base feature set.
The resulting feature schema is the foundation for rolling, delta, scaling, and sequence-building steps.
"""

# 11. FEATURE COLUMN ANALYSIS & CONSTANT COLUMN REMOVAL


print("FD004 FEATURE COLUMN ANALYSIS ")

# ID, settings, sensors (re-derive after earlier drops)
id_cols_fd004      = ["unit", "cycle"]
setting_cols_fd004 = ["setting1", "setting2", "setting3"]
sensor_cols_fd004  = [c for c in train_fd004.columns if c.startswith("sensor")]

print("ID columns      :", id_cols_fd004)
print("Setting columns :", setting_cols_fd004)
print("Sensor columns  :", sensor_cols_fd004)
print("Total sensors   :", len(sensor_cols_fd004))

# find constant columns (zero variance) among settings + sensors
constant_columns_fd004 = []

for col in setting_cols_fd004 + sensor_cols_fd004:
    if train_fd004[col].nunique() == 1:
        constant_columns_fd004.append(col)

print("\nConstant (zero-variance) columns in FD004:", constant_columns_fd004)

# drop constant columns from train + test safely (ignore errors)
train_fd004_clean = train_fd004.drop(columns=constant_columns_fd004, errors="ignore").copy()
test_fd004_clean  = test_fd004.drop(columns=constant_columns_fd004, errors="ignore").copy()

print("\nTRAIN shape before:", train_fd004.shape)
print("TRAIN shape after removing constant cols:", train_fd004_clean.shape)

print("\nTEST shape before :", test_fd004.shape)
print("TEST shape after removing constant cols :", test_fd004_clean.shape)

# collect base feature columns for next steps (exclude id + target + max_cycle)
exclude_cols_fd004_base = ["unit", "cycle", "RUL", "max_cycle"]
feature_cols_fd004 = [
    c for c in train_fd004_clean.columns
    if c not in exclude_cols_fd004_base
]

print("\nBase feature columns count (before rolling/delta):", len(feature_cols_fd004))
print("Sample base feature columns:", feature_cols_fd004[:10])

"""# **12. ROLLING WINDOW FEATURES (3,5 cycles) - fd004**

Add rolling mean/std features for sensors over windows of 3 and 5 cycles per engine.
This captures short-term trends in sensor readings.
These engineered features enrich model understanding of transient degradation patterns without requiring deep sequence lengths.
"""

# 12.  ROLLING WINDOW FEATURES (FD004, 3 & 5 CYCLES)


print("GENERATING ROLLING FEATURES FOR FD004 (3 & 5 cycles) ")

df_fd004_roll = train_fd004_clean.copy()

rolling_windows_fd004 = [3, 5]

# current sensor columns after previous cleaning
sensor_cols_fd004_current = [c for c in df_fd004_roll.columns if c.startswith("sensor")]

# rolling mean + std for each sensor + window size
for w in rolling_windows_fd004:
    for col in sensor_cols_fd004_current:
        # rolling mean
        df_fd004_roll[f"{col}_roll{w}_mean"] = (
            df_fd004_roll.groupby("unit")[col]
            .rolling(window=w, min_periods=1)
            .mean()
            .reset_index(level=0, drop=True)
        )
        # rolling std
        df_fd004_roll[f"{col}_roll{w}_std"] = (
            df_fd004_roll.groupby("unit")[col]
            .rolling(window=w, min_periods=1)
            .std()
            .reset_index(level=0, drop=True)
        )

print("Rolling feature generation complete for FD004.")
new_roll_cols_fd004 = [c for c in df_fd004_roll.columns if "roll" in c]
print("Total rolling columns added:", len(new_roll_cols_fd004))
print("Sample rolling feature columns:", new_roll_cols_fd004[:10])

print("\nUpdated FD004 TRAIN shape after rolling features:", df_fd004_roll.shape)

"""# **13. DELTA FEATURES (sensor_t - sensor_(t-1)) FOR FD004**

Delta features quantify the rate of change of each sensor signal.
These first-order differences help sequence models capture abrupt health changes and improve predictive sensitivity to early-stage degradation.
"""

# 13. DELTA FEATURES (sensor_t - sensor_(t-1)) FOR FD004


print("GENERATING DELTA FEATURES FOR FD004")

df_fd004_delta = df_fd004_roll.copy()
# use original sensor columns (not rolling ones) for delta
sensor_cols_fd004_current = [c for c in train_fd004_clean.columns if c.startswith("sensor")]

# delta = current - previous cycle (per engine)
for col in sensor_cols_fd004_current:
    df_fd004_delta[f"{col}_delta"] = (
        df_fd004_delta.groupby("unit")[col].diff().fillna(0)
    )

new_delta_cols_fd004 = [c for c in df_fd004_delta.columns if c.endswith("_delta")]
print("Delta features added.")
print("Total delta columns added:", len(new_delta_cols_fd004))
print("Sample delta feature columns:", new_delta_cols_fd004[:10])

print("\nUpdated FD004 TRAIN shape after delta features:", df_fd004_delta.shape)

"""# **14. CLEAN NaNs/INFs + FINAL FEATURE SET + SCALING (FD004)**

All engineered features undergo cleaning and MinMax scaling to ensure numerical stability.
This unified scaled feature matrix is required for consistent training across classical and deep learning architectures.
"""

# 14. CLEAN NaNs/INFs + FINAL FEATURE SET + SCALING (FD004)


print("CLEANING & SCALING FEATURE MATRIX (FD004) ")

from sklearn.preprocessing import MinMaxScaler
import numpy as np

df_fd004_final = df_fd004_delta.copy()

# 1. Final feature columns (exclude ID + target-related)
exclude_cols_fd004 = ["unit", "cycle", "max_cycle", "RUL"]
feature_cols_final_fd004 = [
    c for c in df_fd004_final.columns
    if c not in exclude_cols_fd004
]

print(f"Total final feature columns (FD004): {len(feature_cols_final_fd004)}")
print("Sample:", feature_cols_final_fd004[:10])

# 2. Handle inf / NaN from rolling std, etc.
df_fd004_final[feature_cols_final_fd004] = (
    df_fd004_final[feature_cols_final_fd004]
    .replace([np.inf, -np.inf], np.nan)
    .fillna(0.0)
)

# 3. Fit MinMaxScaler on TRAIN features (FD004)
scaler_fd004 = MinMaxScaler()
df_fd004_final[feature_cols_final_fd004] = scaler_fd004.fit_transform(
    df_fd004_final[feature_cols_final_fd004]
)

print("Scaling complete for FD004 feature matrix.")
print("\nFD004 TRAIN final shape (after scaling):", df_fd004_final.shape)
display(df_fd004_final.head())

"""# **15. TRAIN/VALIDATION SPLIT BY ENGINE ID (FD004)**

Engines are split using an 80/20 split at the unit level to prevent leakage.
Splitting by engine ensures each engine appears only once (train or validation), preserving real-world generalization behavior.
"""

# 15. TRAIN/VALIDATION SPLIT BY ENGINE ID (FD004)

print(" TRAIN/VALIDATION SPLIT BY ENGINE ID — FD004 ")

from sklearn.model_selection import train_test_split

# get unique engine IDs from final processed data
engine_ids_fd004 = df_fd004_final["unit"].unique()
print("Total engines in FD004 TRAIN:", len(engine_ids_fd004))

# 80/20 split (by engine, no leakage)
train_ids_fd004, val_ids_fd004 = train_test_split(
    engine_ids_fd004, test_size=0.2, random_state=42
)

print("\nTrain engine count (FD004):", len(train_ids_fd004))
print("Val engine count   (FD004):", len(val_ids_fd004))

# Build split dataframes
train_df_fd004 = df_fd004_final[df_fd004_final["unit"].isin(train_ids_fd004)].copy()
val_df_fd004   = df_fd004_final[df_fd004_final["unit"].isin(val_ids_fd004)].copy()


print("\nFD004 TRAIN DF shape:", train_df_fd004.shape)
print("FD004 VAL DF   shape:", val_df_fd004.shape)

# sanity check: no engine ID overlap between train/val
overlap_fd004 = set(train_ids_fd004).intersection(set(val_ids_fd004))
if overlap_fd004:
    raise ValueError(f"data leakage detected! {len(overlap_fd004)} overlapping engine IDs")
else:
    print("good - no engine ID overlap between train/val.")

"""# **16. CLASSICAL BASELINE MODELS (RF & XGB) — FD004**

Quick RandomForest and lightweight XGBoost baselines using tabular features (no sequences).
Subsampled for speed - just checking how good simple models are before deep learning.
"""

# CELL 16 — CLASSICAL BASELINE MODELS (RF & XGB) — FD004


print("running classical baselines (rf + xgb) on fd004")

from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, mean_absolute_error
import numpy as np

# subsample for faster baselines
MAX_BASELINE_ROWS = 40000

def subsample_df(df, max_rows, seed=SEED):
    if len(df) > max_rows:
        return df.sample(n=max_rows, random_state=seed)
    return df

train_df_fd004_base = subsample_df(train_df_fd004, MAX_BASELINE_ROWS)
val_df_fd004_base   = subsample_df(val_df_fd004,   MAX_BASELINE_ROWS // 2)

print(f"Baseline TRAIN rows used (FD004): {len(train_df_fd004_base)}")
print(f"Baseline VAL   rows used (FD004): {len(val_df_fd004_base)}")

# tabular feature matrices (no sequences)
X_train_base_fd004 = train_df_fd004_base[feature_cols_final_fd004].values
X_val_base_fd004   = val_df_fd004_base[feature_cols_final_fd004].values

y_train_base_fd004 = train_df_fd004_base["RUL"].values
y_val_base_fd004   = val_df_fd004_base["RUL"].values

print("Baseline X_train shape (FD004):", X_train_base_fd004.shape)
print("Baseline X_val   shape (FD004):", X_val_base_fd004.shape)
print("Baseline y_train shape (FD004):", y_train_base_fd004.shape)
print("Baseline y_val   shape (FD004):", y_val_base_fd004.shape)


#  Random Forest baseline

rf_fd004 = RandomForestRegressor(
    n_estimators=150,
    max_depth=20,
    n_jobs=-1,
    random_state=SEED,
    min_samples_split=4,
    min_samples_leaf=2,
)

print("\n[RF] Fitting RandomForest baseline for FD004 ...")
rf_fd004.fit(X_train_base_fd004, y_train_base_fd004)

y_val_pred_rf_fd004 = rf_fd004.predict(X_val_base_fd004)

rmse_rf_fd004 = np.sqrt(mean_squared_error(y_val_base_fd004, y_val_pred_rf_fd004))
mae_rf_fd004  = mean_absolute_error(y_val_base_fd004, y_val_pred_rf_fd004)

print("\nFD004 RANDOM FOREST BASELINE — VALIDATION PERFORMANCE ")
print(f"RMSE (val): {rmse_rf_fd004:.4f}")
print(f"MAE  (val): {mae_rf_fd004:.4f}")


#  XGBoost baseline
try:
    from xgboost import XGBRegressor

    xgb_fd004 = XGBRegressor(
        n_estimators=200,
        max_depth=4,
        learning_rate=0.05,
        subsample=0.8,
        colsample_bytree=0.8,
        random_state=SEED,
        tree_method="hist",
    )

    print("\n[XGB] Fitting XGBoost baseline for FD004 ...")
    xgb_fd004.fit(X_train_base_fd004, y_train_base_fd004)

    y_val_pred_xgb_fd004 = xgb_fd004.predict(X_val_base_fd004)

    rmse_xgb_fd004 = np.sqrt(mean_squared_error(y_val_base_fd004, y_val_pred_xgb_fd004))
    mae_xgb_fd004  = mean_absolute_error(y_val_base_fd004, y_val_pred_xgb_fd004)

    print("\n FD004 XGBOOST BASELINE — VALIDATION PERFORMANCE ")
    print(f"RMSE (val): {rmse_xgb_fd004:.4f}")
    print(f"MAE  (val): {mae_xgb_fd004:.4f}")

except ImportError:
    print("\n[xgboost NOT installed] Skipping XGBoost baseline for FD004.")
    rmse_xgb_fd004 = None
    mae_xgb_fd004  = None

print("\n[INFO] FD004 classical baselines (RF/XGB-LITE) complete.")

"""# **17. Sequence building (seq_len=30) - fd004**

A general-purpose sliding-window sequence constructor builds 30-step sequences for each engine.
This serves as the input format for recurrent neural networks and supports early-stage experimentation with shorter temporal contexts.
"""

# 17. Sequence building (seq_len=30) - fd004


print("Building sequences for fd004 (seq len = 30)")

import numpy as np

SEQ_LEN_FD004_SHORT = 30
TARGET_COL_FD004 = "RUL"

print(f"Sequence length (short): {SEQ_LEN_FD004_SHORT}")
print(f"Number of features per timestep (FD004): {len(feature_cols_final_fd004)}")

def create_sequences_fd004(df, seq_len, feature_cols, target_col):
    """
    Build sliding-window sequences for each engine in FD004.

    X shape: (num_samples, seq_len, num_features)
    y shape: (num_samples,)
    """
    X_list, y_list = [], []

    for unit_id, df_u in df.groupby("unit"):
        df_u_sorted = df_u.sort_values("cycle")
        features = df_u_sorted[feature_cols].values
        target   = df_u_sorted[target_col].values

        if len(df_u_sorted) <= seq_len:
            # Skip engines with very short runs for this seq_len
            continue

        for i in range(len(df_u_sorted) - seq_len):
            X_list.append(features[i : i + seq_len])
            y_list.append(target[i + seq_len])

    return np.array(X_list, dtype=np.float32), np.array(y_list, dtype=np.float32)

# Build TRAIN sequences (seq_len=30)
X_train_seq_30_fd004, y_train_seq_30_fd004 = create_sequences_fd004(
    train_df_fd004,
    SEQ_LEN_FD004_SHORT,
    feature_cols_final_fd004,
    TARGET_COL_FD004
)

# Build VAL sequences (seq_len=30)
X_val_seq_30_fd004, y_val_seq_30_fd004 = create_sequences_fd004(
    val_df_fd004,
    SEQ_LEN_FD004_SHORT,
    feature_cols_final_fd004,
    TARGET_COL_FD004
)

print("\nSequence shapes (seq_len=30):")
print("X_train_seq_30 shape:", X_train_seq_30_fd004.shape)
print("y_train_seq_30 shape:", y_train_seq_30_fd004.shape)
print("X_val_seq_30   shape:", X_val_seq_30_fd004.shape)
print("y_val_seq_30   shape:", y_val_seq_30_fd004.shape)

if X_train_seq_30_fd004.shape[0] == 0 or X_val_seq_30_fd004.shape[0] == 0:
    raise ValueError("No sequences generated for seq_len=30 (FD004). Check data & splits.")

print("\nSequence building done for seq_len=30")

"""# **18. BASELINE LSTM (seq_len = 30) FOR FD004**

Simple 2-layer LSTM baseline with early stopping and learning rate reduction.
Training on seq_len=30 sequences from FD004 train/val split.
"""

# CELL 18 — BASELINE LSTM (seq_len = 30) FOR FD004 + CALLBACKS


print(" FD004 BASELINE LSTM TRAINING (seq_len=30) ")

import tensorflow as tf
from tensorflow.keras import layers, models, callbacks
from sklearn.metrics import mean_squared_error, mean_absolute_error
import numpy as np
import pandas as pd

n_timesteps_30_fd004 = X_train_seq_30_fd004.shape[1]
n_features_fd004     = X_train_seq_30_fd004.shape[2]

print(f"Input shape (FD004, seq_len=30): timesteps={n_timesteps_30_fd004}, features={n_features_fd004}")

# 1. Baseline LSTM architecture
baseline_lstm_30_fd004 = models.Sequential([
    layers.Input(shape=(n_timesteps_30_fd004, n_features_fd004)),
    layers.LSTM(64, return_sequences=True),
    layers.LSTM(32),
    layers.Dense(32, activation="relu"),
    layers.Dense(1)  # regression
])

baseline_lstm_30_fd004.summary()

# 2. Compile
baseline_lstm_30_fd004.compile(
    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),
    loss="mse",
    metrics=[
        tf.keras.metrics.MeanAbsoluteError(name="mae"),
        tf.keras.metrics.RootMeanSquaredError(name="rmse")
    ]
)

# 3. Callbacks (EarlyStopping + ReduceLROnPlateau)
early_stop_30_fd004 = callbacks.EarlyStopping(
    monitor="val_loss",
    patience=5,
    restore_best_weights=True
)

reduce_lr_30_fd004 = callbacks.ReduceLROnPlateau(
    monitor="val_loss",
    factor=0.5,
    patience=3,
    min_lr=1e-5,
    verbose=1
)

# 4. Train
EPOCHS_30_FD004 = 30
BATCH_SIZE_FD004 = 128

history_lstm_30_fd004 = baseline_lstm_30_fd004.fit(
    X_train_seq_30_fd004,
    y_train_seq_30_fd004,
    validation_data=(X_val_seq_30_fd004, y_val_seq_30_fd004),
    epochs=EPOCHS_30_FD004,
    batch_size=BATCH_SIZE_FD004,
    callbacks=[early_stop_30_fd004, reduce_lr_30_fd004],
    verbose=1
)

print("\nBaseline LSTM (FD004, seq_len=30) training complete.")

# 5. Evaluation on validation set
y_val_pred_30_fd004 = baseline_lstm_30_fd004.predict(X_val_seq_30_fd004, verbose=0).reshape(-1)

mse_lstm_30_fd004 = mean_squared_error(y_val_seq_30_fd004, y_val_pred_30_fd004)
rmse_lstm_30_fd004 = np.sqrt(mse_lstm_30_fd004)
mae_lstm_30_fd004  = mean_absolute_error(y_val_seq_30_fd004, y_val_pred_30_fd004)

print("\nFD004 BASELINE LSTM (seq_len=30) — VALIDATION PERFORMANCE")
print(f"RMSE (val): {rmse_lstm_30_fd004:.4f}")
print(f"MAE  (val): {mae_lstm_30_fd004:.4f}")

results_preview_30_fd004 = pd.DataFrame({
    "True_RUL": y_val_seq_30_fd004[:20],
    "Pred_RUL": y_val_pred_30_fd004[:20].round(2)
})

print("\nFirst 20 validation samples — True vs Predicted RUL (FD004, LSTM seq=30):")
display(results_preview_30_fd004)

"""# **19. BUILD SEQUENCES FOR FD004 WITH SEQ_LEN = 100**

Longer temporal windows (100 cycles) are constructed to capture richer degradation trajectories.
These serve as inputs for all advanced deep architectures used later in the pipeline.
"""

# 19. BUILD SEQUENCES FOR FD004 WITH SEQ_LEN = 100


print(" Building sequences for fd004 (seq len = 100)")

SEQ_LEN_FD004_LONG = 100
print(f"Sequence length (long): {SEQ_LEN_FD004_LONG}")

# Reusing create_sequences_fd004 from Cell 17

X_train_seq_100_fd004, y_train_seq_100_fd004 = create_sequences_fd004(
    train_df_fd004,
    SEQ_LEN_FD004_LONG,
    feature_cols_final_fd004,
    TARGET_COL_FD004
)

X_val_seq_100_fd004, y_val_seq_100_fd004 = create_sequences_fd004(
    val_df_fd004,
    SEQ_LEN_FD004_LONG,
    feature_cols_final_fd004,
    TARGET_COL_FD004
)

print("\nSequence shapes (seq_len=100):")
print("X_train_seq_100 shape:", X_train_seq_100_fd004.shape)
print("y_train_seq_100 shape:", y_train_seq_100_fd004.shape)
print("X_val_seq_100   shape:", X_val_seq_100_fd004.shape)
print("y_val_seq_100   shape:", y_val_seq_100_fd004.shape)

if X_train_seq_100_fd004.shape[0] == 0 or X_val_seq_100_fd004.shape[0] == 0:
    raise ValueError("warning: no sequences for seq_len=100 - some engines too short")

print("\nSeq_len=100 building complete")

"""# **20. BASELINE LSTM (seq_len = 100) FOR FD004**

Same LSTM architecture pattern as Cell 18 but trained on longer 100-cycle sequences.
More history should help capture longer degradation patterns.
"""

# 20. BASELINE LSTM (seq_len = 100) FOR FD004


print(" FD004 BASELINE LSTM TRAINING (seq_len=100) ")

import tensorflow as tf
from tensorflow.keras import layers, models, callbacks
from sklearn.metrics import mean_squared_error, mean_absolute_error
import numpy as np
import pandas as pd

n_timesteps_100_fd004 = X_train_seq_100_fd004.shape[1]
n_features_fd004      = X_train_seq_100_fd004.shape[2]

print(f"Input shape (FD004, seq_len=100): timesteps={n_timesteps_100_fd004}, features={n_features_fd004}")

# 1. Baseline LSTM architecture for seq_len=100
baseline_lstm_100_fd004 = models.Sequential([
    layers.Input(shape=(n_timesteps_100_fd004, n_features_fd004)),
    layers.LSTM(128, return_sequences=True),
    layers.Dropout(0.2),
    layers.LSTM(64),
    layers.Dense(32, activation="relu"),
    layers.Dense(1)
])

baseline_lstm_100_fd004.summary()

# 2. Compile
baseline_lstm_100_fd004.compile(
    optimizer=tf.keras.optimizers.Adam(learning_rate=5e-4),
    loss="mse",
    metrics=[
        tf.keras.metrics.MeanAbsoluteError(name="mae"),
        tf.keras.metrics.RootMeanSquaredError(name="rmse")
    ]
)

# 3. Callbacks
early_stop_100_fd004 = callbacks.EarlyStopping(
    monitor="val_loss",
    patience=6,
    restore_best_weights=True
)

reduce_lr_100_fd004 = callbacks.ReduceLROnPlateau(
    monitor="val_loss",
    factor=0.5,
    patience=3,
    min_lr=1e-5,
    verbose=1
)

# 4. Train
EPOCHS_100_FD004 = 40
BATCH_SIZE_FD004 = 128

history_lstm_100_fd004 = baseline_lstm_100_fd004.fit(
    X_train_seq_100_fd004,
    y_train_seq_100_fd004,
    validation_data=(X_val_seq_100_fd004, y_val_seq_100_fd004),
    epochs=EPOCHS_100_FD004,
    batch_size=BATCH_SIZE_FD004,
    callbacks=[early_stop_100_fd004, reduce_lr_100_fd004],
    verbose=1
)

print("\nBaseline LSTM (FD004, seq_len=100) training complete.")

# 5. Evaluation on validation set
y_val_pred_100_fd004 = baseline_lstm_100_fd004.predict(X_val_seq_100_fd004, verbose=0).reshape(-1)

mse_lstm_100_fd004 = mean_squared_error(y_val_seq_100_fd004, y_val_pred_100_fd004)
rmse_lstm_100_fd004 = np.sqrt(mse_lstm_100_fd004)
mae_lstm_100_fd004  = mean_absolute_error(y_val_seq_100_fd004, y_val_pred_100_fd004)

print("\nFD004 BASELINE LSTM (seq_len=100) — VALIDATION PERFORMANCE ")
print(f"RMSE (val): {rmse_lstm_100_fd004:.4f}")
print(f"MAE  (val): {mae_lstm_100_fd004:.4f}")

results_preview_100_fd004 = pd.DataFrame({
    "True_RUL": y_val_seq_100_fd004[:20],
    "Pred_RUL": y_val_pred_100_fd004[:20].round(2)
})

print("\nFirst 20 validation samples — True vs Predicted RUL (FD004, LSTM seq=100):")
display(results_preview_100_fd004)

"""# **21. GRU Baselines (seq_len=30 & 100) for FD004**

Train simple GRU baselines similar to LSTM ones, for both short (30) and long (100) sequences.
Check how GRUs perform compared to LSTMs.
"""

# 21. BASELINE GRU MODELS (seq_len = 30 & 100) — FD004


print("FD004 BASELINE GRU TRAINING (seq_len=30)")

import tensorflow as tf
from tensorflow.keras import layers, models, callbacks
from sklearn.metrics import mean_squared_error, mean_absolute_error
import numpy as np
import pandas as pd


# 1) GRU baseline — seq_len = 30

n_timesteps_30_fd004 = X_train_seq_30_fd004.shape[1]
n_features_fd004     = X_train_seq_30_fd004.shape[2]

gru_30_fd004 = models.Sequential([
    layers.Input(shape=(n_timesteps_30_fd004, n_features_fd004)),
    layers.GRU(64, return_sequences=True),
    layers.GRU(32),
    layers.Dense(32, activation="relu"),
    layers.Dense(1)
])

gru_30_fd004.summary()

gru_30_fd004.compile(
    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),
    loss="mse",
    metrics=[
        tf.keras.metrics.MeanAbsoluteError(name="mae"),
        tf.keras.metrics.RootMeanSquaredError(name="rmse"),
    ],
)

early_stop_gru_30 = callbacks.EarlyStopping(
    monitor="val_loss",
    patience=5,
    restore_best_weights=True
)

reduce_lr_gru_30 = callbacks.ReduceLROnPlateau(
    monitor="val_loss",
    factor=0.5,
    patience=3,
    min_lr=1e-5,
    verbose=1
)

EPOCHS_GRU_30_FD004 = 30
BATCH_SIZE_FD004    = 128

history_gru_30_fd004 = gru_30_fd004.fit(
    X_train_seq_30_fd004,
    y_train_seq_30_fd004,
    validation_data=(X_val_seq_30_fd004, y_val_seq_30_fd004),
    epochs=EPOCHS_GRU_30_FD004,
    batch_size=BATCH_SIZE_FD004,
    callbacks=[early_stop_gru_30, reduce_lr_gru_30],
    verbose=1
)

y_val_pred_gru_30_fd004 = gru_30_fd004.predict(X_val_seq_30_fd004, verbose=0).reshape(-1)
rmse_gru_30_fd004 = np.sqrt(mean_squared_error(y_val_seq_30_fd004, y_val_pred_gru_30_fd004))
mae_gru_30_fd004  = mean_absolute_error(y_val_seq_30_fd004, y_val_pred_gru_30_fd004)

print("\n FD004 BASELINE GRU (seq_len=30) — VALIDATION PERFORMANCE ")
print(f"RMSE (val): {rmse_gru_30_fd004:.4f}")
print(f"MAE  (val): {mae_gru_30_fd004:.4f}")

# preview
preview_gru_30_fd004 = pd.DataFrame({
    "True_RUL": y_val_seq_30_fd004[:20],
    "Pred_RUL_GRU_30": y_val_pred_gru_30_fd004[:20].round(2),
})
display(preview_gru_30_fd004)


# 2) GRU baseline — seq_len = 100

print("\n FD004 BASELINE GRU TRAINING (seq_len=100) ")

n_timesteps_100_fd004 = X_train_seq_100_fd004.shape[1]
# n_features_fd004 is same as before

gru_100_fd004 = models.Sequential([
    layers.Input(shape=(n_timesteps_100_fd004, n_features_fd004)),
    layers.GRU(128, return_sequences=True),
    layers.Dropout(0.2),
    layers.GRU(64),
    layers.Dense(32, activation="relu"),
    layers.Dense(1),
])

gru_100_fd004.summary()

gru_100_fd004.compile(
    optimizer=tf.keras.optimizers.Adam(learning_rate=5e-4),
    loss="mse",
    metrics=[
        tf.keras.metrics.MeanAbsoluteError(name="mae"),
        tf.keras.metrics.RootMeanSquaredError(name="rmse"),
    ],
)

early_stop_gru_100 = callbacks.EarlyStopping(
    monitor="val_loss",
    patience=6,
    restore_best_weights=True
)

reduce_lr_gru_100 = callbacks.ReduceLROnPlateau(
    monitor="val_loss",
    factor=0.5,
    patience=3,
    min_lr=1e-5,
    verbose=1
)

EPOCHS_GRU_100_FD004 = 40

history_gru_100_fd004 = gru_100_fd004.fit(
    X_train_seq_100_fd004,
    y_train_seq_100_fd004,
    validation_data=(X_val_seq_100_fd004, y_val_seq_100_fd004),
    epochs=EPOCHS_GRU_100_FD004,
    batch_size=BATCH_SIZE_FD004,
    callbacks=[early_stop_gru_100, reduce_lr_gru_100],
    verbose=1
)

y_val_pred_gru_100_fd004 = gru_100_fd004.predict(X_val_seq_100_fd004, verbose=0).reshape(-1)
rmse_gru_100_fd004 = np.sqrt(mean_squared_error(y_val_seq_100_fd004, y_val_pred_gru_100_fd004))
mae_gru_100_fd004  = mean_absolute_error(y_val_seq_100_fd004, y_val_pred_gru_100_fd004)

print("\n FD004 BASELINE GRU (seq_len=100) — VALIDATION PERFORMANCE ")
print(f"RMSE (val): {rmse_gru_100_fd004:.4f}")
print(f"MAE  (val): {mae_gru_100_fd004:.4f}")

preview_gru_100_fd004 = pd.DataFrame({
    "True_RUL": y_val_seq_100_fd004[:20],
    "Pred_RUL_GRU_100": y_val_pred_gru_100_fd004[:20].round(2),
})
display(preview_gru_100_fd004)

"""# **22. RUL Target Normalization (MinMaxScaler) for FD004 (seq_len=100)**

RUL values are scaled to [0,1] to stabilize training for tuned deep networks.
Scaling is essential for models with sensitive gradient dynamics such as tuned LSTMs/GRUs and hybrid architectures.
"""

# 22. Normalize RUL Target (RUL Scaling Using MinMaxScaler) — FD004

from sklearn.preprocessing import MinMaxScaler

print("NORMALIZING RUL TARGET VALUES (FD004, seq_len=100) ")

rul_scaler_fd004 = MinMaxScaler()

# Fit on TRAIN RUL only (for seq_len=100)
y_train_100_scaled_fd004 = rul_scaler_fd004.fit_transform(
    y_train_seq_100_fd004.reshape(-1, 1)
)
y_val_100_scaled_fd004 = rul_scaler_fd004.transform(
    y_val_seq_100_fd004.reshape(-1, 1)
)

print("RUL scaling complete for FD004 (seq_len=100).")
print("Sample original vs scaled RUL (first 5):")
for orig, scaled in zip(y_train_seq_100_fd004[:5], y_train_100_scaled_fd004[:5]):
    print(f"RUL {orig:6.1f} -> scaled {scaled[0]:.6f}")

"""# **23. Tuned LSTM (seq_len=100, Scaled RUL) for FD004**

A tuned LSTM architecture with dropout regularization and optimized hyperparameters is trained using scaled targets.
This model usually serves as a strong baseline for FD004.
"""

# 23. Tuned LSTM (seq_len = 100, SCALED RUL) — FD004


print(" TUNED LSTM (SCALED RUL) TRAINING — FD004 (seq_len=100)")

n_timesteps_100_fd004 = X_train_seq_100_fd004.shape[1]
n_features_fd004      = X_train_seq_100_fd004.shape[2]

tuned_lstm_100_fd004 = models.Sequential([
    layers.Input(shape=(n_timesteps_100_fd004, n_features_fd004)),
    layers.LSTM(128, return_sequences=True),
    layers.Dropout(0.3),
    layers.LSTM(64, return_sequences=False),
    layers.Dropout(0.3),
    layers.Dense(64, activation="relu"),
    layers.Dense(1, activation="linear"),
])

tuned_lstm_100_fd004.summary()

tuned_lstm_100_fd004.compile(
    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),
    loss="mse",
    metrics=[tf.keras.metrics.MeanAbsoluteError(name="mae")],
)

early_stop_lstm_tuned_fd004 = callbacks.EarlyStopping(
    monitor="val_loss",
    patience=6,
    restore_best_weights=True
)

reduce_lr_lstm_tuned_fd004 = callbacks.ReduceLROnPlateau(
    monitor="val_loss",
    factor=0.5,
    patience=3,
    min_lr=1e-5,
    verbose=1
)

EPOCHS_LSTM_TUNED_100_FD004 = 50

history_lstm_tuned_100_fd004 = tuned_lstm_100_fd004.fit(
    X_train_seq_100_fd004,
    y_train_100_scaled_fd004,        # SCALED target
    validation_data=(X_val_seq_100_fd004, y_val_100_scaled_fd004),
    epochs=EPOCHS_LSTM_TUNED_100_FD004,
    batch_size=BATCH_SIZE_FD004,
    callbacks=[early_stop_lstm_tuned_fd004, reduce_lr_lstm_tuned_fd004],
    verbose=1
)

print("\nTuned LSTM (scaled RUL, seq_len=100) training complete.")

"""# **24. Evaluate Tuned LSTM (Real RUL Units) — FD004**

Predictions are inverse-scaled back to real RUL.
Validation metrics (RMSE/MAE) are computed to ensure training-scale transformations do not introduce distortions.
"""

# 24. Tuned LSTM Validation Metrics in REAL RUL Units — FD004


print(" TUNED LSTM (FD004, seq_len=100) — VALIDATION PERFORMANCE (REAL RUL) ")

# 1) Predict scaled RUL
y_val_pred_lstm_100_scaled_fd004 = tuned_lstm_100_fd004.predict(
    X_val_seq_100_fd004, verbose=0
)

# 2) Inverse-scale back to REAL RUL
y_val_pred_lstm_100_real_fd004 = rul_scaler_fd004.inverse_transform(
    y_val_pred_lstm_100_scaled_fd004
).reshape(-1)

y_val_real_fd004 = y_val_seq_100_fd004

# 3) Metrics
rmse_lstm_tuned_100_fd004 = np.sqrt(
    mean_squared_error(y_val_real_fd004, y_val_pred_lstm_100_real_fd004)
)
mae_lstm_tuned_100_fd004 = mean_absolute_error(
    y_val_real_fd004, y_val_pred_lstm_100_real_fd004
)

print(f"RMSE (val, Tuned LSTM 100): {rmse_lstm_tuned_100_fd004:.4f}")
print(f"MAE  (val, Tuned LSTM 100): {mae_lstm_tuned_100_fd004:.4f}")

preview_lstm_tuned_100_fd004 = pd.DataFrame({
    "True_RUL": y_val_real_fd004[:20],
    "Pred_RUL_LSTM_TUNED_100": y_val_pred_lstm_100_real_fd004[:20].round(2),
})
display(preview_lstm_tuned_100_fd004.head(20))

"""# **25. Tuned GRU (seq_len=100, Scaled RUL)**

A tuned GRU network is trained similarly to the tuned LSTM.
GRUs often outperform LSTMs for FD004 because FD004 contains mixed operating conditions.
"""

# 25. Tuned GRU (seq_len = 100, SCALED RUL)


print(" TUNED GRU (SCALED RUL) TRAINING — FD004 (seq_len=100) ")

tuned_gru_100_fd004 = models.Sequential([
    layers.Input(shape=(n_timesteps_100_fd004, n_features_fd004)),
    layers.GRU(128, return_sequences=True),
    layers.Dropout(0.3),
    layers.GRU(64, return_sequences=False),
    layers.Dropout(0.3),
    layers.Dense(64, activation="relu"),
    layers.Dense(1, activation="linear"),
])

tuned_gru_100_fd004.summary()

tuned_gru_100_fd004.compile(
    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),
    loss="mse",
    metrics=[tf.keras.metrics.MeanAbsoluteError(name="mae")],
)

early_stop_gru_tuned_fd004 = callbacks.EarlyStopping(
    monitor="val_loss",
    patience=6,
    restore_best_weights=True
)

reduce_lr_gru_tuned_fd004 = callbacks.ReduceLROnPlateau(
    monitor="val_loss",
    factor=0.5,
    patience=3,
    min_lr=1e-5,
    verbose=1
)

EPOCHS_GRU_TUNED_100_FD004 = 50

history_gru_tuned_100_fd004 = tuned_gru_100_fd004.fit(
    X_train_seq_100_fd004,
    y_train_100_scaled_fd004,
    validation_data=(X_val_seq_100_fd004, y_val_100_scaled_fd004),
    epochs=EPOCHS_GRU_TUNED_100_FD004,
    batch_size=BATCH_SIZE_FD004,
    callbacks=[early_stop_gru_tuned_fd004, reduce_lr_gru_tuned_fd004],
    verbose=1
)

print("\nTuned GRU (scaled RUL, seq_len=100) training complete.")

"""# **26. Evaluate Tuned GRU (Real RUL Units) — FD004**

Scaled predictions are inverse-transformed and evaluated.
Results provide model comparability across architectures.
"""

# 26. Tuned GRU Validation Metrics in REAL RUL Units — FD004


print(" TUNED GRU (FD004, seq_len=100) — VALIDATION PERFORMANCE (REAL RUL)")

y_val_pred_gru_100_scaled_fd004 = tuned_gru_100_fd004.predict(
    X_val_seq_100_fd004, verbose=0
)

y_val_pred_gru_100_real_fd004 = rul_scaler_fd004.inverse_transform(
    y_val_pred_gru_100_scaled_fd004
).reshape(-1)

rmse_gru_tuned_100_fd004 = np.sqrt(
    mean_squared_error(y_val_real_fd004, y_val_pred_gru_100_real_fd004)
)
mae_gru_tuned_100_fd004 = mean_absolute_error(
    y_val_real_fd004, y_val_pred_gru_100_real_fd004
)

print(f"RMSE (val, Tuned GRU 100): {rmse_gru_tuned_100_fd004:.4f}")
print(f"MAE  (val, Tuned GRU 100): {mae_gru_tuned_100_fd004:.4f}")

preview_gru_tuned_100_fd004 = pd.DataFrame({
    "True_RUL": y_val_real_fd004[:20],
    "Pred_RUL_GRU_TUNED_100": y_val_pred_gru_100_real_fd004[:20].round(2),
})
display(preview_gru_tuned_100_fd004.head(20))

"""# **27. CNN-LSTM (seq_len=100, Scaled RUL) for FD004**

A convolutional front-end extracts local sensor patterns before feeding into LSTMs.
This hybrid architecture usually excels in CMAPSS due to its ability to learn both short-term and long-term patterns simultaneously.
"""

# 27. CNN-LSTM (seq_len = 100, SCALED RUL) — FD004


print(" CNN-LSTM (SCALED RUL, seq_len=100) TRAINING — FD004")

cnn_lstm_100_fd004 = models.Sequential([
    layers.Input(shape=(n_timesteps_100_fd004, n_features_fd004)),

    # 1D Conv block
    layers.Conv1D(filters=64, kernel_size=3, padding="same", activation="relu"),
    layers.Conv1D(filters=64, kernel_size=3, padding="same", activation="relu"),
    layers.MaxPooling1D(pool_size=2),

    # LSTM block
    layers.LSTM(64, return_sequences=True),
    layers.LSTM(32, return_sequences=False),

    # Dense head
    layers.Dense(64, activation="relu"),
    layers.Dropout(0.3),
    layers.Dense(1, activation="linear"),
])

cnn_lstm_100_fd004.summary()

cnn_lstm_100_fd004.compile(
    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),
    loss="mse",
    metrics=[tf.keras.metrics.MeanAbsoluteError(name="mae")],
)

early_stop_cnnlstm_fd004 = callbacks.EarlyStopping(
    monitor="val_loss",
    patience=6,
    restore_best_weights=True
)

reduce_lr_cnnlstm_fd004 = callbacks.ReduceLROnPlateau(
    monitor="val_loss",
    factor=0.5,
    patience=3,
    min_lr=1e-5,
    verbose=1
)

EPOCHS_CNNLSTM_100_FD004 = 50

history_cnn_lstm_100_fd004 = cnn_lstm_100_fd004.fit(
    X_train_seq_100_fd004,
    y_train_100_scaled_fd004,
    validation_data=(X_val_seq_100_fd004, y_val_100_scaled_fd004),
    epochs=EPOCHS_CNNLSTM_100_FD004,
    batch_size=BATCH_SIZE_FD004,
    callbacks=[early_stop_cnnlstm_fd004, reduce_lr_cnnlstm_fd004],
    verbose=1
)

print("\nCNN-LSTM (scaled RUL, seq_len=100) training complete.")

"""# **28. Evaluate CNN-LSTM (Real RUL Units) — FD004**

The CNN-LSTM model is evaluated in real RUL units, enabling comparison against LSTM, GRU, and hybrid variants.
"""

# 28. CNN-LSTM Validation Metrics in REAL RUL Units — FD004


print(" CNN-LSTM (FD004, seq_len=100) — VALIDATION PERFORMANCE (REAL RUL)")

y_val_pred_cnnlstm_100_scaled_fd004 = cnn_lstm_100_fd004.predict(
    X_val_seq_100_fd004, verbose=0
)

y_val_pred_cnnlstm_100_real_fd004 = rul_scaler_fd004.inverse_transform(
    y_val_pred_cnnlstm_100_scaled_fd004
).reshape(-1)

rmse_cnn_lstm_100_fd004 = np.sqrt(
    mean_squared_error(y_val_real_fd004, y_val_pred_cnnlstm_100_real_fd004)
)
mae_cnn_lstm_100_fd004 = mean_absolute_error(
    y_val_real_fd004, y_val_pred_cnnlstm_100_real_fd004
)

print(f"RMSE (val, CNN-LSTM 100): {rmse_cnn_lstm_100_fd004:.4f}")
print(f"MAE  (val, CNN-LSTM 100): {mae_cnn_lstm_100_fd004:.4f}")

preview_cnn_lstm_100_fd004 = pd.DataFrame({
    "True_RUL": y_val_real_fd004[:20],
    "Pred_RUL_CNN_LSTM_100": y_val_pred_cnnlstm_100_real_fd004[:20].round(2),
})
display(preview_cnn_lstm_100_fd004.head(20))

"""# **29. Hybrid LSTM–GRU (seq_len=100, Scaled RUL) for FD004**

A stacked LSTM → GRU architecture is trained to jointly capture long-range memory (LSTM) and efficient gating (GRU).
This model often performs among the best in CMAPSS FD004 scenarios.
"""

# 29. Hybrid LSTM–GRU (seq_len = 100, SCALED RUL) — FD004


print(" HYBRID LSTM–GRU (SCALED RUL, seq_len=100) TRAINING — FD004 ")

hybrid_100_fd004 = models.Sequential([
    layers.Input(shape=(n_timesteps_100_fd004, n_features_fd004)),

    # First LSTM block
    layers.LSTM(128, return_sequences=True),
    layers.Dropout(0.3),

    # GRU block
    layers.GRU(64, return_sequences=False),
    layers.Dropout(0.3),

    # Dense head
    layers.Dense(64, activation="relu"),
    layers.Dense(32, activation="relu"),
    layers.Dense(1, activation="linear"),
])

hybrid_100_fd004.summary()

hybrid_100_fd004.compile(
    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),
    loss="mse",
    metrics=[tf.keras.metrics.MeanAbsoluteError(name="mae")],
)

early_stop_hybrid_fd004 = callbacks.EarlyStopping(
    monitor="val_loss",
    patience=6,
    restore_best_weights=True
)

reduce_lr_hybrid_fd004 = callbacks.ReduceLROnPlateau(
    monitor="val_loss",
    factor=0.5,
    patience=3,
    min_lr=1e-5,
    verbose=1
)

EPOCHS_HYBRID_100_FD004 = 50

history_hybrid_100_fd004 = hybrid_100_fd004.fit(
    X_train_seq_100_fd004,
    y_train_100_scaled_fd004,
    validation_data=(X_val_seq_100_fd004, y_val_100_scaled_fd004),
    epochs=EPOCHS_HYBRID_100_FD004,
    batch_size=BATCH_SIZE_FD004,
    callbacks=[early_stop_hybrid_fd004, reduce_lr_hybrid_fd004],
    verbose=1
)

print("\nHybrid LSTM–GRU (scaled RUL, seq_len=100) training complete.")

"""# **30. Evaluate Hybrid LSTM–GRU (Real RUL Units) — FD004**

Real RUL predictions are evaluated for completeness and leaderboard ranking.
"""

# 30. Hybrid LSTM–GRU Validation Metrics in REAL RUL Units — FD004


print(" HYBRID LSTM–GRU (FD004, seq_len=100) — VALIDATION PERFORMANCE (REAL RUL) ")

y_val_pred_hybrid_100_scaled_fd004 = hybrid_100_fd004.predict(
    X_val_seq_100_fd004, verbose=0
)

y_val_pred_hybrid_100_real_fd004 = rul_scaler_fd004.inverse_transform(
    y_val_pred_hybrid_100_scaled_fd004
).reshape(-1)

rmse_hybrid_100_fd004 = np.sqrt(
    mean_squared_error(y_val_real_fd004, y_val_pred_hybrid_100_real_fd004)
)
mae_hybrid_100_fd004 = mean_absolute_error(
    y_val_real_fd004, y_val_pred_hybrid_100_real_fd004
)

print(f"RMSE (val, HYBRID 100): {rmse_hybrid_100_fd004:.4f}")
print(f"MAE  (val, HYBRID 100): {mae_hybrid_100_fd004:.4f}")

preview_hybrid_100_fd004 = pd.DataFrame({
    "True_RUL": y_val_real_fd004[:20],
    "Pred_RUL_HYBRID_100": y_val_pred_hybrid_100_real_fd004[:20].round(2),
})
display(preview_hybrid_100_fd004.head(20))

"""# **31. FD004 Model Leaderboard (Validation, Real RUL)**

All classical and deep models are ranked based on validation RMSE/MAE.
This leaderboard determines the best-performing FD004 model for NASA scoring and inference.
"""

# 31. FD004 MODEL LEADERBOARD (VALIDATION, REAL RUL)


print(" FD004 MODEL LEADERBOARD (Validation, REAL RUL) ")

results_fd004 = []

def add_model_fd004(name, seq_len, family, rmse_val, mae_val):
    if rmse_val is None or mae_val is None:
        return
    results_fd004.append({
        "Model": name,
        "Seq_Len": seq_len,
        "Family": family,
        "RMSE_val": float(rmse_val),
        "MAE_val": float(mae_val),
    })

# Classical
add_model_fd004("RF_BASE",  None, "CLASSICAL", rmse_rf_fd004,     mae_rf_fd004)
add_model_fd004("XGB_BASE", None, "CLASSICAL", rmse_xgb_fd004,    mae_xgb_fd004)

# Deep baselines
add_model_fd004("LSTM_30_BASE",   30, "LSTM", rmse_lstm_30_fd004,   mae_lstm_30_fd004)
add_model_fd004("LSTM_100_BASE", 100, "LSTM", rmse_lstm_100_fd004,  mae_lstm_100_fd004)
add_model_fd004("GRU_30_BASE",    30, "GRU",  rmse_gru_30_fd004,    mae_gru_30_fd004)
add_model_fd004("GRU_100_BASE",  100, "GRU",  rmse_gru_100_fd004,   mae_gru_100_fd004)

# Tuned 100-length models (scaled RUL)
add_model_fd004("LSTM_100_TUNED", 100, "LSTM_TUNED",
                rmse_lstm_tuned_100_fd004, mae_lstm_tuned_100_fd004)
add_model_fd004("GRU_100_TUNED",  100, "GRU_TUNED",
                rmse_gru_tuned_100_fd004,  mae_gru_tuned_100_fd004)
add_model_fd004("CNN_LSTM_100",   100, "CNN_LSTM",
                rmse_cnn_lstm_100_fd004,   mae_cnn_lstm_100_fd004)
add_model_fd004("HYBRID_100",     100, "HYBRID",
                rmse_hybrid_100_fd004,     mae_hybrid_100_fd004)

df_fd004_leaderboard = (
    pd.DataFrame(results_fd004)
    .sort_values("RMSE_val")
    .reset_index(drop=True)
)

display(df_fd004_leaderboard)

best_row_fd004 = df_fd004_leaderboard.iloc[0]
best_model_name_fd004 = best_row_fd004["Model"]

print("\n Best FD004 model by validation RMSE:")
print(best_row_fd004)

"""# **32. TEST FEATURE ENGINEERING (ROLLING + DELTA + SCALING) — FD004**

The test set undergoes identical transformations as the training set to maintain feature alignment.
This includes rolling features, delta features, NaN cleaning, and MinMax scaling.
"""

# 32. TEST FEATURE ENGINEERING (ROLLING + DELTA + SCALING) — FD004


print("FD004 TEST FEATURE ENGINEERING (ROLLING + DELTA + SCALING) ")

import numpy as np

# We start from test_fd004_clean (created in Cell 11)
df_fd004_test = test_fd004_clean.copy()

# 1) Rolling features (same windows and sensors as TRAIN)
rolling_windows_fd004 = [3, 5]

sensor_cols_fd004_current = [c for c in df_fd004_test.columns if c.startswith("sensor")]

print("Using sensor columns for TEST:", sensor_cols_fd004_current)

for w in rolling_windows_fd004:
    for col in sensor_cols_fd004_current:
        df_fd004_test[f"{col}_roll{w}_mean"] = (
            df_fd004_test.groupby("unit")[col]
            .rolling(window=w, min_periods=1)
            .mean()
            .reset_index(level=0, drop=True)
        )
        df_fd004_test[f"{col}_roll{w}_std"] = (
            df_fd004_test.groupby("unit")[col]
            .rolling(window=w, min_periods=1)
            .std()
            .reset_index(level=0, drop=True)
        )

print("Rolling features added to FD004 TEST.")

# 2) Delta features (sensor_t - sensor_(t-1))
for col in sensor_cols_fd004_current:
    df_fd004_test[f"{col}_delta"] = (
        df_fd004_test.groupby("unit")[col].diff().fillna(0.0)
    )

print("Delta features added to FD004 TEST.")

print("\nFD004 TEST shape after FE (before cleaning):", df_fd004_test.shape)

# 3) Keep only columns that exist in TRAIN final feature set + ID columns
id_cols_fd004 = ["unit", "cycle"]
required_cols_fd004 = id_cols_fd004 + feature_cols_final_fd004

# Safety: if any feature in feature_cols_final_fd004 is missing in test, raise error
missing_test_features = [c for c in feature_cols_final_fd004 if c not in df_fd004_test.columns]
if missing_test_features:
    raise ValueError(f" FD004 TEST missing feature columns (mismatch with TRAIN): {missing_test_features}")

df_fd004_test_final = df_fd004_test[required_cols_fd004].copy()

# 4) Clean infs/NaNs
df_fd004_test_final[feature_cols_final_fd004] = (
    df_fd004_test_final[feature_cols_final_fd004]
    .replace([np.inf, -np.inf], np.nan)
    .fillna(0.0)
)

# 5) Apply TRAIN scaler_fd004 to TEST features
X_test_scaled_fd004 = scaler_fd004.transform(
    df_fd004_test_final[feature_cols_final_fd004].values
)

print("\nFD004 TEST feature matrix scaled.")
print("X_test_scaled_fd004 shape:", X_test_scaled_fd004.shape)
display(df_fd004_test_final.head())

"""# **33. BUILD TEST LAST-WINDOW SEQUENCES (seq_len=100) — FD004**

For NASA test scoring: take LAST 100 cycles from each test engine as input sequence.
Target RUL comes from RUL_FD004.txt (one value per engine).
"""

# 33. BUILD TEST LAST-WINDOW SEQUENCES (seq_len=100) — FD004


print("FD004 — BUILD TEST LAST-WINDOW SEQUENCES (seq_len = 100) ")

SEQ_LEN_FD004_NASA = 100  # We use 100-step history for best deep model

def build_test_last_window_sequences_fd004(
    df_test_base,
    X_scaled_test,
    feature_cols,
    rul_vector,
    seq_len
):
    """
    For each test engine in FD004:
      - Sort by cycle
      - Take the LAST `seq_len` rows as one sequence
      - Target RUL from RUL_FD004.txt (NASA rule: one RUL per engine)
    """
    X_list = []
    y_list = []

    units = sorted(df_test_base["unit"].unique())
    print(f"Total TEST engines in FD004: {len(units)}")

    # Make sure X_scaled_test aligns with df_test_base row order
    # (df_fd004_test_final is the base for X_test_scaled_fd004)
    df_temp = df_test_base.reset_index(drop=True)
    feature_array = X_scaled_test

    for unit_id in units:
        mask = df_temp["unit"] == unit_id
        df_u = df_temp[mask].sort_values("cycle")
        X_u = feature_array[mask.values]

        if len(df_u) < seq_len:
            # Skip engines that are too short
            continue

        seq = X_u[-seq_len:, :]  # last `seq_len` timesteps

        # NASA RUL vector: engine 1 -> index 0, engine 2 -> index 1, etc.
        true_rul = float(rul_vector.iloc[unit_id - 1])

        X_list.append(seq)
        y_list.append(true_rul)

    X_arr = np.array(X_list, dtype=np.float32)
    y_arr = np.array(y_list, dtype=np.float32)
    return X_arr, y_arr

X_test_seq_100_fd004, y_test_last_true_fd004 = build_test_last_window_sequences_fd004(
    df_fd004_test_final,
    X_test_scaled_fd004,
    feature_cols_final_fd004,
    rul_fd004,
    SEQ_LEN_FD004_NASA,
)

print("\nFD004 TEST last-window sequence shapes (seq_len=100):")
print("X_test_seq_100_fd004 shape:", X_test_seq_100_fd004.shape)
print("y_test_last_true_fd004 shape:", y_test_last_true_fd004.shape)

if X_test_seq_100_fd004.shape[0] == 0:
    raise ValueError(" No FD004 TEST sequences built for seq_len=100 — check data lengths.")

"""# **34. NASA ASYMMETRIC SCORE FUNCTION — FD004**

NASA CMAPSS scoring: overestimation (late failure prediction) penalized more heavily than underestimation.
Used for final test evaluation and model calibration.
"""

# 34. NASA ASYMMETRIC SCORE FUNCTION — FD004


print(" DEFINING NASA ASYMMETRIC SCORE FUNCTION (FD004) ")

import numpy as np

def nasa_asymmetric_score(y_true, y_pred):
    """
    NASA CMAPSS-style asymmetric scoring:
    - Overestimation (late) is penalized more than underestimation (early).
    """
    y_true = np.asarray(y_true, dtype=float)
    y_pred = np.asarray(y_pred, dtype=float)

    e = y_pred - y_true  # positive = predicting too high (late)

    score = np.zeros_like(e, dtype=float)

    mask_under = e < 0   # early predictions (less severe)
    score[mask_under] = np.exp(-e[mask_under] / 13.0) - 1.0

    mask_over = e >= 0   # late predictions (more severe)
    score[mask_over] = np.exp(e[mask_over] / 10.0) - 1.0

    return float(score.sum()), float(score.mean())

def clip_rul(x, max_rul):
    x = np.asarray(x, dtype=float)
    return np.clip(x, 0.0, max_rul)

"""# **35. SELECT BEST DEEP MODEL & NASA CALIBRATION (VALIDATION) — FD004**

Pick best seq_len=100 deep model by val RMSE, then calibrate NASA shift/cap parameters on validation set.
Prepares for final test NASA scoring.
"""

# 35. SELECT BEST DEEP MODEL & NASA CALIBRATION (VALIDATION) — FD004


print(" FD004 NASA CALIBRATION ON VALIDATION (DEEP MODELS ONLY) ")

# We restrict to deep seq_len=100 models for NASA  and ignore classical RF/XGB
df_fd004_deep_100 = df_fd004_leaderboard[
    (df_fd004_leaderboard["Family"] != "CLASSICAL") &
    (df_fd004_leaderboard["Seq_Len"] == 100)
].copy()

if df_fd004_deep_100.empty:
    raise RuntimeError(" No deep 100-step models found for NASA calibration in FD004.")

df_fd004_deep_100 = df_fd004_deep_100.sort_values("RMSE_val").reset_index(drop=True)
best_deep_row_fd004 = df_fd004_deep_100.iloc[0]
best_deep_model_name_fd004 = best_deep_row_fd004["Model"]

print("Best FD004 deep model (for NASA):")
print(best_deep_row_fd004)


# Helper to get model object and info (scaled vs unscaled)

def get_fd004_model_and_config(name: str):
    """
    Maps model name to:
      - model object
      - whether it was trained with scaled RUL ('scaled') or real RUL ('unscaled')
      - which seq_len and data arrays it expects
    """
    if name == "LSTM_100_BASE":
        return baseline_lstm_100_fd004, "unscaled", 100
    elif name == "GRU_100_BASE":
        return gru_100_fd004, "unscaled", 100
    elif name == "LSTM_100_TUNED":
        return tuned_lstm_100_fd004, "scaled",   100
    elif name == "GRU_100_TUNED":
        return tuned_gru_100_fd004,  "scaled",   100
    elif name == "CNN_LSTM_100":
        return cnn_lstm_100_fd004,   "scaled",   100
    elif name == "HYBRID_100":
        return hybrid_100_fd004,     "scaled",   100
    else:
        raise ValueError(f"Unknown FD004 deep model name for NASA: {name}")

best_model_fd004, best_model_target_type_fd004, best_model_seq_len_fd004 = get_fd004_model_and_config(
    best_deep_model_name_fd004
)

print(f"\nUsing model '{best_deep_model_name_fd004}' for NASA calibration.")
print(f"Target type: {best_model_target_type_fd004}, seq_len: {best_model_seq_len_fd004}")


# 1) Get raw validation predictions in REAL RUL units

if best_model_seq_len_fd004 != 100:
    raise ValueError("This code assumes best deep model uses seq_len=100 for NASA.")

X_val_for_nasa = X_val_seq_100_fd004
y_val_for_nasa = y_val_seq_100_fd004  # real RUL

# a) Predict
val_pred_raw_fd004 = best_model_fd004.predict(X_val_for_nasa, verbose=0)

# b) Convert to REAL RUL units if model was trained on scaled RUL
if best_model_target_type_fd004 == "scaled":
    val_pred_real_fd004 = rul_scaler_fd004.inverse_transform(val_pred_raw_fd004).reshape(-1)
else:
    val_pred_real_fd004 = val_pred_raw_fd004.reshape(-1)

print("\nSample validation predictions (real RUL) for NASA calibration:")
print(val_pred_real_fd004[:10])


# 2) NASA calibration: try different shifts and caps

shifts = np.arange(-30, 31, 1)  # -30 to +30 cycles
max_rul_candidates = [110, 120, 125, 130]

best_mean_nasa_fd004 = np.inf
BEST_NASA_SHIFT_FD004 = 0.0
MAX_RUL_NASA_FD004 = 125.0

for cap in max_rul_candidates:
    for s in shifts:
        y_calib = clip_rul(val_pred_real_fd004 - s, max_rul=cap)
        _, mean_score = nasa_asymmetric_score(y_val_for_nasa, y_calib)
        if mean_score < best_mean_nasa_fd004:
            best_mean_nasa_fd004 = mean_score
            BEST_NASA_SHIFT_FD004 = float(s)
            MAX_RUL_NASA_FD004 = float(cap)

print("\n=== FD004 NASA CALIBRATION RESULTS (VALIDATION) ===")
print(f"Best mean NASA score (val) : {best_mean_nasa_fd004:.4f}")
print(f"Best shift (cycles)        : {BEST_NASA_SHIFT_FD004}")
print(f"Best max RUL cap           : {MAX_RUL_NASA_FD004}")
print("\n Calibration complete — will reuse these for TEST NASA scoring.")

"""# **36. FD004 TEST PREDICTIONS (RAW + CALIBRATED) + NASA SCORE**

This step generates the final RUL predictions for the FD004 test engines and evaluates them using NASA’s asymmetric scoring metric.
First, raw model predictions are produced and converted to real RUL units if needed.
Then the NASA score is calculated for the raw outputs.
Next, the predictions are calibrated using the optimal shift and RUL cap identified earlier, and the NASA score is recomputed.
This provides both raw and calibrated test performance, with the calibrated NASA score serving as the final benchmark result for FD004.
"""

# 36. FD004 TEST PREDICTIONS (RAW + CALIBRATED) + NASA SCORE


print(" FD004 TEST PREDICTIONS + NASA SCORES (RAW + CALIBRATED) ")

# 1) Raw predictions on TEST last-window sequences (scaled or unscaled)
y_test_pred_raw_fd004 = best_model_fd004.predict(X_test_seq_100_fd004, verbose=0)

# 2) Convert to REAL RUL units if needed
if best_model_target_type_fd004 == "scaled":
    y_test_pred_real_fd004 = rul_scaler_fd004.inverse_transform(
        y_test_pred_raw_fd004
    ).reshape(-1)
else:
    y_test_pred_real_fd004 = y_test_pred_raw_fd004.reshape(-1)

# 3) NASA score — RAW predictions
sum_nasa_raw_fd004, mean_nasa_raw_fd004 = nasa_asymmetric_score(
    y_test_last_true_fd004,
    y_test_pred_real_fd004,
)

print("\n FD004 NASA SCORE (TEST, RAW PREDICTIONS) ")
print(f"Total NASA score (raw) : {sum_nasa_raw_fd004:.4f}")
print(f"Mean NASA score  (raw) : {mean_nasa_raw_fd004:.4f}")

# 4) Apply calibration (shift + cap) and recompute NASA
y_test_pred_calibrated_fd004 = clip_rul(
    y_test_pred_real_fd004 - BEST_NASA_SHIFT_FD004,
    max_rul=MAX_RUL_NASA_FD004,
)

sum_nasa_cal_fd004, mean_nasa_cal_fd004 = nasa_asymmetric_score(
    y_test_last_true_fd004,
    y_test_pred_calibrated_fd004,
)

print("\n FD004 NASA SCORE (TEST, CALIBRATED) ")
print(f"Total NASA score (cal) : {sum_nasa_cal_fd004:.4f}")
print(f"Mean NASA score  (cal) : {mean_nasa_cal_fd004:.4f}")

"""# **37. FD004 TEST OUTPUT TABLE (TRUE vs RAW/CALIBRATED RUL)**

This section compiles a clear comparison table showing the true RUL values for each FD004 test engine alongside the model’s raw predictions and calibrated predictions.
The table allows quick inspection of prediction accuracy and the impact of calibration.
A preview of the first 20 engines is displayed for validation and reporting purposes.
"""

# 37. FD004 TEST OUTPUT TABLE (TRUE vs RAW/CALIBRATED RUL)


print(" FD004 TEST RUL PREDICTION TABLE (USING BEST DEEP MODEL) ")

import pandas as pd
import numpy as np

num_engines_fd004_test = len(y_test_last_true_fd004)
units_fd004_test = np.arange(1, num_engines_fd004_test + 1)

df_fd004_test_out = pd.DataFrame({
    "unit": units_fd004_test,
    "True_RUL": y_test_last_true_fd004,
    "Predicted_RUL_RAW": y_test_pred_real_fd004,
    "Predicted_RUL_CALIBRATED": y_test_pred_calibrated_fd004,
})

print("\n SAMPLE FD004 TEST PREDICTIONS (first 20 engines) ")
display(df_fd004_test_out.head(20))

"""# **38. SAVE FD004 NASA SCORE + TEST RESULTS**

This step saves all key FD004 evaluation outputs for reproducibility and external analysis.
The final NASA scores (raw and calibrated) are written to a text file along with the best model details and calibration settings.
Additionally, the complete test prediction table—containing true RUL, raw predictions, and calibrated predictions—is exported as a CSV file.
These artifacts serve as the final deliverables for reporting, benchmarking, and portfolio documentation.
"""

# 38. SAVE FD004 NASA SCORE + TEST RESULTS


print(" SAVING FD004 NASA SCORES & TEST RESULTS ")

import os

# Base directory for FD004 models & results
MODELS_DIR_FD004 = os.path.join(BASE_PATH_FD004, "models_fd004")
os.makedirs(MODELS_DIR_FD004, exist_ok=True)

# 1) Save NASA scores to text file
nasa_score_path_fd004 = os.path.join(BASE_PATH_FD004, "FD004_NASA_SCORE.txt")
with open(nasa_score_path_fd004, "w") as f:
    f.write(f"Best NASA model name      : {best_deep_model_name_fd004}\n")
    f.write(f"FD004 NASA Score (RAW)    : {sum_nasa_raw_fd004:.4f}\n")
    f.write(f"FD004 NASA Score (CAL)    : {sum_nasa_cal_fd004:.4f}\n")
    f.write(f"FD004 Mean NASA (RAW)     : {mean_nasa_raw_fd004:.4f}\n")
    f.write(f"FD004 Mean NASA (CAL)     : {mean_nasa_cal_fd004:.4f}\n")
    f.write(f"Best NASA shift (cycles)  : {BEST_NASA_SHIFT_FD004}\n")
    f.write(f"Best NASA max RUL cap     : {MAX_RUL_NASA_FD004}\n")

print(f" FD004 NASA scores saved to:\n{nasa_score_path_fd004}")

# 2) Save TEST prediction table as CSV
fd004_test_results_path = os.path.join(BASE_PATH_FD004, "FD004_TEST_RESULTS.csv")
df_fd004_test_out.to_csv(fd004_test_results_path, index=False)

print(f" FD004 TEST predictions saved to:\n{fd004_test_results_path}")

"""# **39. SAVE BEST FD004 DEEP MODEL + SCALERS + CONFIG**

This final step packages all essential components of the FD004 modeling pipeline for deployment and future reuse.
The best-performing deep model is saved in Keras format, along with the feature scaler and RUL scaler used during preprocessing.
A configuration JSON file is also generated, capturing model metadata, feature column definitions, calibration settings, and key validation/test metrics.
These artifacts together ensure full reproducibility of the FD004 workflow and provide a ready-to-deploy model bundle.
"""

# 39. SAVE BEST FD004 DEEP MODEL + SCALERS + CONFIG


print(" SAVING BEST FD004 DEEP MODEL + SCALERS + CONFIG ")

import joblib
import json
import os

# Ensure directory exists
MODELS_DIR_FD004 = os.path.join(BASE_PATH_FD004, "models_fd004")
os.makedirs(MODELS_DIR_FD004, exist_ok=True)

# 1) Save Keras model
best_model_filename_fd004 = f"FD004_{best_deep_model_name_fd004}_BEST.keras"
best_model_path_fd004 = os.path.join(MODELS_DIR_FD004, best_model_filename_fd004)

best_model_fd004.save(best_model_path_fd004)
print(f" Saved best FD004 deep model to:\n{best_model_path_fd004}")

# 2) Save feature scaler and RUL scaler
feature_scaler_path_fd004 = os.path.join(MODELS_DIR_FD004, "FD004_feature_scaler_fd004.pkl")
rul_scaler_path_fd004     = os.path.join(MODELS_DIR_FD004, "FD004_rul_scaler_fd004.pkl")

joblib.dump(scaler_fd004, feature_scaler_path_fd004)
joblib.dump(rul_scaler_fd004, rul_scaler_path_fd004)

print(f" Saved FD004 feature scaler to:\n{feature_scaler_path_fd004}")
print(f" Saved FD004 RUL scaler to    :\n{rul_scaler_path_fd004}")

# 3) Save FD004 configuration JSON
fd004_config = {
    "best_deep_model_name": best_deep_model_name_fd004,
    "best_deep_model_path": best_model_path_fd004,
    "best_deep_model_target_type": best_model_target_type_fd004,
    "sequence_length_nasa": SEQ_LEN_FD004_NASA,
    "feature_scaler_path": feature_scaler_path_fd004,
    "rul_scaler_path": rul_scaler_path_fd004,
    "base_path": BASE_PATH_FD004,
    # raw feature columns before rolling/delta
    "base_feature_columns": feature_cols_fd004,
    # final engineered feature columns used for training (same as TRAIN)
    "final_feature_columns": feature_cols_final_fd004,
    # NASA calibration settings
    "nasa_shift": BEST_NASA_SHIFT_FD004,
    "nasa_max_rul_cap": MAX_RUL_NASA_FD004,
    # Validation / test NASA scores
    "nasa_best_mean_val": best_mean_nasa_fd004,
    "nasa_test_total_raw": sum_nasa_raw_fd004,
    "nasa_test_mean_raw": mean_nasa_raw_fd004,
    "nasa_test_total_cal": sum_nasa_cal_fd004,
    "nasa_test_mean_cal": mean_nasa_cal_fd004,
}

fd004_config_path = os.path.join(MODELS_DIR_FD004, "FD004_config.json")
with open(fd004_config_path, "w") as f:
    json.dump(fd004_config, f, indent=4)

print(f"\n FD004 configuration JSON saved to:\n{fd004_config_path}")

print("\n FD004 SUMMARY (FINAL) ")
print(f"Best deep model (NASA)   : {best_deep_model_name_fd004}")
print(f"Validation RMSE (best)   : {best_deep_row_fd004['RMSE_val']:.4f}")
print(f"Validation MAE  (best)   : {best_deep_row_fd004['MAE_val']:.4f}")
print(f"NASA Mean (TEST, RAW)    : {mean_nasa_raw_fd004:.4f}")
print(f"NASA Mean (TEST, CAL)    : {mean_nasa_cal_fd004:.4f}")
print(f"NASA Shift used          : {BEST_NASA_SHIFT_FD004}")
print(f"NASA Max RUL Cap used    : {MAX_RUL_NASA_FD004}")

print("\n FD004 full pipeline complete: TRAIN, VAL, TEST, NASA, and model saving are all done.")

